{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1503102-04e5-4da2-b079-086f08185efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports & configuration\n",
    "import os, json, math, time\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.signal import welch, spectrogram\n",
    "from scipy.stats import skew, kurtosis, entropy\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Configuration\n",
    "FILES = {\n",
    "    'OFF': 'Motor_OFF.xlsx',\n",
    "    'ON': 'Motor_ON.xlsx',\n",
    "    'NO_FAN': 'Motor_NO_FAN.xlsx'\n",
    "}\n",
    "\n",
    "LABEL_MAP = {'OFF': 0, 'ON': 1, 'NO_FAN': 2}\n",
    "WINDOW_SIZE = 256       # samples per window\n",
    "STEP = 64               # INCREASED OVERLAP: more windows for training\n",
    "DEFAULT_FS = 1000.0     # sampling frequency (Hz)\n",
    "FIG_DIR = \"./figs_improved\"\n",
    "os.makedirs(FIG_DIR, exist_ok=True)\n",
    "RNG = 42\n",
    "np.random.seed(RNG)\n",
    "tf.random.set_seed(RNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634cf159-1cbc-4eda-9d5f-299f3c82be3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA LOADING\n",
    "def load_data(path):\n",
    "\n",
    "    df = pd.read_excel(path, engine='openpyxl')\n",
    "    \n",
    "    expected_cols = ['timestamp', 'Ax', 'Ay', 'Az', 'Gx', 'Gy', 'Gz']\n",
    "    if not all(col in df.columns for col in expected_cols):\n",
    "        raise ValueError(f\"File {path} missing expected columns. Found: {df.columns.tolist()}\")\n",
    "    \n",
    "    df = df[expected_cols].copy()\n",
    "\n",
    "    try:\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    except Exception:\n",
    "        df['timestamp'] = np.arange(len(df))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def infer_fs_from_timestamp(df):\n",
    "    try:\n",
    "        ts = pd.to_datetime(df['timestamp'])\n",
    "        dt = (ts - ts.shift(1)).dropna().dt.total_seconds().values\n",
    "        if len(dt) == 0:\n",
    "            return DEFAULT_FS\n",
    "        median_dt = np.median(dt)\n",
    "        if median_dt <= 0:\n",
    "            return DEFAULT_FS\n",
    "        return 1.0 / median_dt\n",
    "    except Exception:\n",
    "        return DEFAULT_FS\n",
    "\n",
    "def compute_vector_magnitude(df):\n",
    "    ax = df['Ax'].to_numpy(dtype=float)\n",
    "    ay = df['Ay'].to_numpy(dtype=float)\n",
    "    az = df['Az'].to_numpy(dtype=float)\n",
    "    gx = df['Gx'].to_numpy(dtype=float)\n",
    "    gy = df['Gy'].to_numpy(dtype=float)\n",
    "    gz = df['Gz'].to_numpy(dtype=float)\n",
    "    \n",
    "    df['accel_mag'] = np.sqrt(ax*ax + ay*ay + az*az)\n",
    "    df['gyro_mag'] = np.sqrt(gx*gx + gy*gy + gz*gz)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cc8f80-aa5b-49b6-af4e-a5db7e3984ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATASETS\n",
    "\n",
    "data_dfs = {}\n",
    "fs_map = {}\n",
    "print(\"Loading files...\")\n",
    "for label, path in FILES.items():\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Missing file: {path} - place it in working dir\")\n",
    "    df = load_data(path)\n",
    "    df = compute_vector_magnitude(df)\n",
    "    data_dfs[label] = df\n",
    "    fs_map[label] = infer_fs_from_timestamp(df)\n",
    "    print(f\"Loaded {label}: {df.shape}, inferred fs={fs_map[label]:.2f} Hz\")\n",
    "\n",
    "FS = float(np.median(list(fs_map.values())))\n",
    "print(f\"Using FS = {FS} Hz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4204dad-29df-438c-a2e5-0b1b7d2da3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA FUNCTIONS\n",
    "\n",
    "def plot_time_channels(df, label, n_samples=2000, fs=FS, save=False):\n",
    "    n = min(len(df), n_samples)\n",
    "    t = np.arange(n) / fs\n",
    "    fig, axs = plt.subplots(3, 2, figsize=(14, 8))\n",
    "    axs = axs.flatten()\n",
    "    cols = ['Ax', 'Ay', 'Az', 'Gx', 'Gy', 'Gz']\n",
    "    for i, ch in enumerate(cols):\n",
    "        axs[i].plot(t, df[ch].values[:n], lw=0.8)\n",
    "        axs[i].set_title(f\"{label} - {ch}\")\n",
    "        axs[i].set_xlabel(\"time (s)\")\n",
    "    plt.tight_layout()\n",
    "    if save:\n",
    "        plt.savefig(os.path.join(FIG_DIR, f\"{label}_raw_timeseries.png\"), dpi=200)\n",
    "    plt.show()\n",
    "\n",
    "def plot_magnitude_signals(df, label, n_samples=2000, fs=FS, save=False):\n",
    "    n = min(len(df), n_samples)\n",
    "    t = np.arange(n) / fs\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    plt.plot(t, df['accel_mag'].values[:n], label='accel_mag', lw=0.9)\n",
    "    plt.plot(t, df['gyro_mag'].values[:n], label='gyro_mag', lw=0.9)\n",
    "    plt.title(f\"{label} - magnitude signals\")\n",
    "    plt.xlabel(\"time (s)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    if save:\n",
    "        plt.savefig(os.path.join(FIG_DIR, f\"{label}_magnitudes.png\"), dpi=200)\n",
    "    plt.show()\n",
    "\n",
    "def summary_stats(df, label):\n",
    "    print(f\"\\nSummary stats for {label}:\")\n",
    "    for grp, cols in [('Accel', ['Ax', 'Ay', 'Az']), ('Gyro', ['Gx', 'Gy', 'Gz'])]:\n",
    "        arr = df[cols].to_numpy()\n",
    "        print(f\" {grp} mean: {np.round(arr.mean(axis=0), 5)}, std: {np.round(arr.std(axis=0), 5)}\")\n",
    "    print(f\" accel_mag mean/std: {df['accel_mag'].mean():.5f}, {df['accel_mag'].std():.5f}\")\n",
    "    print(f\" gyro_mag mean/std: {df['gyro_mag'].mean():.5f}, {df['gyro_mag'].std():.5f}\")\n",
    "    print(f\" n_samples: {len(df)}\")\n",
    "\n",
    "# Run EDA\n",
    "for label, df in data_dfs.items():\n",
    "    summary_stats(df, label)\n",
    "    plot_time_channels(df, label, n_samples=2000, save=True)\n",
    "    plot_magnitude_signals(df, label, n_samples=2000, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7760fb76-a859-4c44-87e3-d3a18c391937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENHANCED FEATURE ENGINEERING\n",
    "\n",
    "def cov_eigenvalues(window):\n",
    "    #Covariance eigenvalues (orientation-invariant).\"\"\"\n",
    "    C = np.cov(window.T)\n",
    "    eigs = np.linalg.eigvalsh(C)\n",
    "    return np.sort(eigs)[::-1]\n",
    "\n",
    "def make_windows_and_features(df, window_size=WINDOW_SIZE, step=STEP, fs=FS):\n",
    "    # Extract all 6 channels\n",
    "    arr_acc = df[['Ax', 'Ay', 'Az']].to_numpy(dtype=np.float32)\n",
    "    arr_gyro = df[['Gx', 'Gy', 'Gz']].to_numpy(dtype=np.float32)\n",
    "    mag_acc = df['accel_mag'].to_numpy(dtype=np.float32)\n",
    "    mag_gyro = df['gyro_mag'].to_numpy(dtype=np.float32)\n",
    "    \n",
    "    n = arr_acc.shape[0]\n",
    "    windows_ts = []\n",
    "    feat_list = []\n",
    "    \n",
    "    for start in range(0, n - window_size + 1, step):\n",
    "        a_win = arr_acc[start:start+window_size]      # ws x 3\n",
    "        g_win = arr_gyro[start:start+window_size]     # ws x 3\n",
    "        am_win = mag_acc[start:start+window_size]     # ws\n",
    "        gm_win = mag_gyro[start:start+window_size]    # ws\n",
    "        \n",
    "        ts = np.concatenate([a_win, g_win], axis=1)   # ws x 6\n",
    "        windows_ts.append(ts)\n",
    "        \n",
    "        feats = []\n",
    "        \n",
    "        for v in (am_win, gm_win):\n",
    "            feats += [\n",
    "                v.mean(), v.std(), np.sqrt(np.mean(v*v)), \n",
    "                v.max() - v.min(), skew(v), kurtosis(v)\n",
    "            ]\n",
    "        \n",
    "        a_eigs = cov_eigenvalues(a_win)\n",
    "        g_eigs = cov_eigenvalues(g_win)\n",
    "        feats += a_eigs.tolist() + g_eigs.tolist()\n",
    "        \n",
    "        for v in (am_win, gm_win):\n",
    "            N = len(v)\n",
    "            S = np.abs(np.fft.rfft(v * np.hanning(N)))\n",
    "            freqs = np.fft.rfftfreq(N, d=1.0/fs)\n",
    "            if S.sum() == 0:\n",
    "                dom = 0.0\n",
    "                centroid = 0.0\n",
    "            else:\n",
    "                dom = freqs[np.argmax(S)]\n",
    "                centroid = (S * freqs).sum() / S.sum()\n",
    "            feats += [dom, centroid]\n",
    "        \n",
    "        bands = [(0, 50), (50, 150), (150, 300), (300, fs/2)]\n",
    "        for v in (am_win, gm_win):\n",
    "            N = len(v)\n",
    "            S = np.abs(np.fft.rfft(v * np.hanning(N)))**2\n",
    "            freqs = np.fft.rfftfreq(N, d=1.0/fs)\n",
    "            for (fmin, fmax) in bands:\n",
    "                idx = np.where((freqs >= fmin) & (freqs < fmax))[0]\n",
    "                feats.append(S[idx].sum() if len(idx) > 0 else 0.0)\n",
    "        \n",
    "        if len(am_win) > 0 and len(gm_win) > 0:\n",
    "            corr = np.corrcoef(am_win, gm_win)[0, 1]\n",
    "            feats.append(corr if not np.isnan(corr) else 0.0)\n",
    "        else:\n",
    "            feats.append(0.0)\n",
    "        \n",
    "        for v in (am_win, gm_win):\n",
    "            hist, _ = np.histogram(v, bins=20, density=True)\n",
    "            hist = hist + 1e-12  # avoid log(0)\n",
    "            feats.append(entropy(hist))\n",
    "        \n",
    "        for v in (am_win, gm_win):\n",
    "            mean_v = v.mean()\n",
    "            std_v = v.std()\n",
    "            threshold = mean_v + 1.5 * std_v\n",
    "            peaks = np.sum((v[1:-1] > v[:-2]) & (v[1:-1] > v[2:]) & (v[1:-1] > threshold))\n",
    "            feats.append(peaks)\n",
    "        \n",
    "        accel_energy = np.sum(am_win**2)\n",
    "        gyro_energy = np.sum(gm_win**2)\n",
    "        ratio = gyro_energy / (accel_energy + 1e-12)\n",
    "        feats.append(ratio)\n",
    "        \n",
    "        feat_list.append(np.array(feats, dtype=np.float32))\n",
    "    \n",
    "    X_ts = np.stack(windows_ts, axis=0)\n",
    "    X_feat = np.stack(feat_list, axis=0)\n",
    "    return X_ts, X_feat\n",
    "\n",
    "X_parts_ts = []\n",
    "X_parts_feat = []\n",
    "y_parts = []\n",
    "\n",
    "for label, df in data_dfs.items():\n",
    "    X_ts, X_feat = make_windows_and_features(df, window_size=WINDOW_SIZE, step=STEP, fs=FS)\n",
    "    X_parts_ts.append(X_ts)\n",
    "    X_parts_feat.append(X_feat)\n",
    "    y_parts.append(np.full((X_ts.shape[0],), LABEL_MAP[label], dtype=int))\n",
    "    print(f\"{label}: windows={X_ts.shape[0]}, ts_shape={X_ts.shape}, feat_shape={X_feat.shape}\")\n",
    "\n",
    "X_ts = np.concatenate(X_parts_ts, axis=0)\n",
    "X_feat = np.concatenate(X_parts_feat, axis=0)\n",
    "y = np.concatenate(y_parts, axis=0)\n",
    "print(f\"\\nTotal windows: {X_ts.shape}, Feature matrix: {X_feat.shape}\")\n",
    "print(f\"Label distribution: {Counter(y)}\")\n",
    "\n",
    "# Shuffle\n",
    "X_ts, X_feat, y = shuffle(X_ts, X_feat, y, random_state=RNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5d6372-44ed-46b1-9020-e310c911bfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/val/test split\n",
    "X_train_ts, X_temp_ts, X_train_feat, X_temp_feat, y_train, y_temp = train_test_split(\n",
    "    X_ts, X_feat, y, test_size=0.30, stratify=y, random_state=RNG\n",
    ")\n",
    "X_val_ts, X_test_ts, X_val_feat, X_test_feat, y_val, y_test = train_test_split(\n",
    "    X_temp_ts, X_temp_feat, y_temp, test_size=0.50, stratify=y_temp, random_state=RNG\n",
    ")\n",
    "print(f\"Train/Val/Test: {X_train_ts.shape[0]}/{X_val_ts.shape[0]}/{X_test_ts.shape[0]}\")\n",
    "\n",
    "# Standardize features\n",
    "feat_scaler = StandardScaler().fit(X_train_feat)\n",
    "X_train_feat = feat_scaler.transform(X_train_feat)\n",
    "X_val_feat = feat_scaler.transform(X_val_feat)\n",
    "X_test_feat = feat_scaler.transform(X_test_feat)\n",
    "\n",
    "# Standardize time-series (per channel)\n",
    "for ch in range(X_train_ts.shape[2]):\n",
    "    scaler = StandardScaler().fit(X_train_ts[:, :, ch])\n",
    "    X_train_ts[:, :, ch] = scaler.transform(X_train_ts[:, :, ch])\n",
    "    X_val_ts[:, :, ch] = scaler.transform(X_val_ts[:, :, ch])\n",
    "    X_test_ts[:, :, ch] = scaler.transform(X_test_ts[:, :, ch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77b3d3b-1a14-470a-848b-47e443160967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA AUGMENTATION\n",
    "def augment_ts(window):\n",
    "    w = window.copy()\n",
    "    \n",
    "    # Jitter\n",
    "    if np.random.rand() < 0.7:\n",
    "        w += np.random.normal(0, 0.02, w.shape)\n",
    "    \n",
    "    # Scaling per channel\n",
    "    if np.random.rand() < 0.5:\n",
    "        scale = np.random.normal(1.0, 0.08, (1, w.shape[1]))\n",
    "        w = w * scale\n",
    "    \n",
    "    # Time shift\n",
    "    if np.random.rand() < 0.4:\n",
    "        shift = int(np.random.uniform(-0.1, 0.1) * w.shape[0])\n",
    "        w = np.roll(w, shift, axis=0)\n",
    "    \n",
    "    # Small rotation (mix channels slightly - simulates orientation change)\n",
    "    if np.random.rand() < 0.3:\n",
    "        angle = np.random.uniform(-0.1, 0.1)\n",
    "        cos_a, sin_a = np.cos(angle), np.sin(angle)\n",
    "        # Rotate accel x-y\n",
    "        w[:, [0, 1]] = w[:, [0, 1]] @ np.array([[cos_a, -sin_a], [sin_a, cos_a]])\n",
    "        # Rotate gyro x-y\n",
    "        w[:, [3, 4]] = w[:, [3, 4]] @ np.array([[cos_a, -sin_a], [sin_a, cos_a]])\n",
    "    \n",
    "    return w\n",
    "\n",
    "def balance_and_augment(X_ts_arr, X_feat_arr, y_arr, target_count=None):\n",
    "    uniq, counts = np.unique(y_arr, return_counts=True)\n",
    "    if target_count is None:\n",
    "        target_count = int(max(counts) * 1.5)  # Oversample more aggressively\n",
    "    \n",
    "    pieces_ts, pieces_feat, pieces_y = [], [], []\n",
    "    \n",
    "    for cls in uniq:\n",
    "        idxs = np.where(y_arr == cls)[0]\n",
    "        pieces_ts.append(X_ts_arr[idxs])\n",
    "        pieces_feat.append(X_feat_arr[idxs])\n",
    "        pieces_y.append(y_arr[idxs])\n",
    "        \n",
    "        n_cur = len(idxs)\n",
    "        if n_cur < target_count:\n",
    "            need = target_count - n_cur\n",
    "            choices = np.random.choice(idxs, size=need, replace=True)\n",
    "            aug_ts = np.stack([augment_ts(X_ts_arr[c]) for c in choices], axis=0)\n",
    "            aug_feat = X_feat_arr[choices]\n",
    "            pieces_ts.append(aug_ts)\n",
    "            pieces_feat.append(aug_feat)\n",
    "            pieces_y.append(np.full((need,), cls, dtype=int))\n",
    "    \n",
    "    Xb_ts = np.concatenate(pieces_ts, axis=0)\n",
    "    Xb_feat = np.concatenate(pieces_feat, axis=0)\n",
    "    yb = np.concatenate(pieces_y, axis=0)\n",
    "    perm = np.random.permutation(len(yb))\n",
    "    return Xb_ts[perm], Xb_feat[perm], yb[perm]\n",
    "\n",
    "print(f\"\\nOriginal train label counts: {Counter(y_train)}\")\n",
    "Xb_train_ts, Xb_train_feat, yb_train = balance_and_augment(X_train_ts, X_train_feat, y_train)\n",
    "print(f\"Balanced train label counts: {Counter(yb_train)}\")\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(yb_train), y=yb_train)\n",
    "class_weight_dict = {i: w for i, w in enumerate(class_weights)}\n",
    "print(f\"Class weights: {class_weight_dict}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6adc87c-a4aa-4200-afbf-9f0499d4642f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL ARCHITECTURE\n",
    "\n",
    "def focal_loss_tf(gamma=2.0, alpha=0.25):\n",
    "    def loss(y_true, y_pred):\n",
    "        y_true_onehot = tf.one_hot(tf.cast(tf.reshape(y_true, [-1]), tf.int32), \n",
    "                                    depth=tf.shape(y_pred)[-1])\n",
    "        p_t = tf.reduce_sum(y_true_onehot * y_pred, axis=-1)\n",
    "        p_t = tf.clip_by_value(p_t, 1e-8, 1.0)\n",
    "        loss_val = -alpha * tf.pow(1.0 - p_t, gamma) * tf.math.log(p_t)\n",
    "        return tf.reduce_mean(loss_val)\n",
    "    return loss\n",
    "\n",
    "def build_improved_model(input_ts_shape, input_feat_dim, num_classes, l2_reg=5e-4):\n",
    "    # Time-series branch (6 channels)\n",
    "    ts_in = keras.Input(shape=input_ts_shape, name='ts_input')\n",
    "    x = ts_in\n",
    "    \n",
    "    # Conv blocks with residuals\n",
    "    for filters, kernel in [(64, 5), (96, 3), (128, 3), (128, 3)]:\n",
    "        y = layers.Conv1D(filters, kernel, padding='same', activation=None,\n",
    "                         kernel_regularizer=keras.regularizers.l2(l2_reg))(x)\n",
    "        y = layers.BatchNormalization()(y)\n",
    "        y = layers.Activation('relu')(y)\n",
    "        y = layers.Conv1D(filters, kernel, padding='same', activation=None,\n",
    "                         kernel_regularizer=keras.regularizers.l2(l2_reg))(y)\n",
    "        y = layers.BatchNormalization()(y)\n",
    "        \n",
    "        # Projection if needed\n",
    "        if x.shape[-1] != filters:\n",
    "            x = layers.Conv1D(filters, 1, padding='same')(x)\n",
    "        \n",
    "        x = layers.Add()([x, y])\n",
    "        x = layers.Activation('relu')(x)\n",
    "        x = layers.Dropout(0.2)(x)\n",
    "        x = layers.MaxPooling1D(2)(x)\n",
    "    \n",
    "    # Simple attention mechanism\n",
    "    attention = layers.Dense(1, activation='tanh')(x)\n",
    "    attention = layers.Softmax(axis=1)(attention)\n",
    "    x_attended = layers.Multiply()([x, attention])\n",
    "    x = layers.GlobalAveragePooling1D()(x_attended)\n",
    "    \n",
    "    # Feature branch\n",
    "    feat_in = keras.Input(shape=(input_feat_dim,), name='feat_input')\n",
    "    f = layers.Dense(256, activation='relu', \n",
    "                    kernel_regularizer=keras.regularizers.l2(l2_reg))(feat_in)\n",
    "    f = layers.BatchNormalization()(f)\n",
    "    f = layers.Dropout(0.3)(f)\n",
    "    f = layers.Dense(128, activation='relu',\n",
    "                    kernel_regularizer=keras.regularizers.l2(l2_reg))(f)\n",
    "    f = layers.BatchNormalization()(f)\n",
    "    f = layers.Dropout(0.2)(f)\n",
    "    \n",
    "    # Fusion\n",
    "    concat = layers.Concatenate()([x, f])\n",
    "    h = layers.Dense(256, activation='relu',\n",
    "                    kernel_regularizer=keras.regularizers.l2(l2_reg))(concat)\n",
    "    h = layers.BatchNormalization()(h)\n",
    "    h = layers.Dropout(0.4)(h)\n",
    "    h = layers.Dense(128, activation='relu',\n",
    "                    kernel_regularizer=keras.regularizers.l2(l2_reg))(h)\n",
    "    h = layers.Dropout(0.3)(h)\n",
    "    \n",
    "    out = layers.Dense(num_classes, activation='softmax')(h)\n",
    "    \n",
    "    model = keras.Model(inputs=[ts_in, feat_in], outputs=out, name='improved_hybrid')\n",
    "    return model\n",
    "\n",
    "# Build model\n",
    "input_ts_shape = Xb_train_ts.shape[1:]  # (window_size, 6)\n",
    "input_feat_dim = Xb_train_feat.shape[1]\n",
    "num_classes = len(np.unique(yb_train))\n",
    "\n",
    "model = build_improved_model(input_ts_shape, input_feat_dim, num_classes)\n",
    "model.summary()\n",
    "print(f\"\\nTotal params: {model.count_params()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4baf959a-c28d-4c68-8594-fe08792578ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "loss_fn = focal_loss_tf(gamma=2.0, alpha=0.25)\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=5e-4),\n",
    "    loss=loss_fn,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.5, patience=8, min_lr=1e-6, verbose=1\n",
    "    ),\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=20, restore_best_weights=True, verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"\\nTraining model...\")\n",
    "history = model.fit(\n",
    "    [Xb_train_ts, Xb_train_feat], yb_train,\n",
    "    validation_data=([X_val_ts, X_val_feat], y_val),\n",
    "    epochs=150,\n",
    "    batch_size=32,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=callbacks,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2a6ea4-fea7-4ba3-96df-b9286986f56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATION\n",
    "\n",
    "# Training curves\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='train_loss', alpha=0.8)\n",
    "plt.plot(history.history['val_loss'], label='val_loss', alpha=0.8)\n",
    "plt.legend()\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='train_acc', alpha=0.8)\n",
    "plt.plot(history.history.get('val_accuracy', []), label='val_acc', alpha=0.8)\n",
    "plt.legend()\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIG_DIR, 'training_curves.png'), dpi=200)\n",
    "plt.show()\n",
    "\n",
    "# Test evaluation\n",
    "test_loss, test_acc = model.evaluate([X_test_ts, X_test_feat], y_test, verbose=0)\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"TEST RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Test loss: {test_loss:.4f}\")\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "\n",
    "y_prob = model.predict([X_test_ts, X_test_feat])\n",
    "y_pred = np.argmax(y_prob, axis=1)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=list(LABEL_MAP.keys()), \n",
    "                          zero_division=0))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Confusion matrix heatmap\n",
    "plt.figure(figsize=(6, 5))\n",
    "cm_norm = cm.astype(float) / (cm.sum(axis=1, keepdims=True) + 1e-12)\n",
    "sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Blues',\n",
    "            xticklabels=list(LABEL_MAP.keys()), \n",
    "            yticklabels=list(LABEL_MAP.keys()),\n",
    "            cbar_kws={'label': 'Proportion'})\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Normalized Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIG_DIR, 'confusion_matrix.png'), dpi=200)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
